{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **PART 1 : IMPORT The LIBRARIES**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fadc85497046766f"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T20:33:46.028056300Z",
     "start_time": "2024-06-13T20:33:45.941192700Z"
    }
   },
   "id": "22d2737b4d547217"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "       Tweet ID       entity sentiment   \n0          2401  Borderlands  Positive  \\\n1          2401  Borderlands  Positive   \n2          2401  Borderlands  Positive   \n3          2401  Borderlands  Positive   \n4          2401  Borderlands  Positive   \n...         ...          ...       ...   \n74676      9200       Nvidia  Positive   \n74677      9200       Nvidia  Positive   \n74678      9200       Nvidia  Positive   \n74679      9200       Nvidia  Positive   \n74680      9200       Nvidia  Positive   \n\n                                           Tweet content  \n0      I am coming to the borders and I will kill you...  \n1      im getting on borderlands and i will kill you ...  \n2      im coming on borderlands and i will murder you...  \n3      im getting on borderlands 2 and i will murder ...  \n4      im getting into borderlands and i can murder y...  \n...                                                  ...  \n74676  Just realized that the Windows partition of my...  \n74677  Just realized that my Mac window partition is ...  \n74678  Just realized the windows partition of my Mac ...  \n74679  Just realized between the windows partition of...  \n74680  Just like the windows partition of my Mac is l...  \n\n[74681 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet ID</th>\n      <th>entity</th>\n      <th>sentiment</th>\n      <th>Tweet content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>I am coming to the borders and I will kill you...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands and i will kill you ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im coming on borderlands and i will murder you...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands 2 and i will murder ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting into borderlands and i can murder y...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>74676</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized that the Windows partition of my...</td>\n    </tr>\n    <tr>\n      <th>74677</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized that my Mac window partition is ...</td>\n    </tr>\n    <tr>\n      <th>74678</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized the windows partition of my Mac ...</td>\n    </tr>\n    <tr>\n      <th>74679</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized between the windows partition of...</td>\n    </tr>\n    <tr>\n      <th>74680</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just like the windows partition of my Mac is l...</td>\n    </tr>\n  </tbody>\n</table>\n<p>74681 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the Twitter dataset\n",
    "df = pd.read_csv('datasets/twitter_training.csv')\n",
    "# naming column names\n",
    "df.columns = ['Tweet ID', 'entity', 'sentiment', 'Tweet content']\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T20:33:50.575471900Z",
     "start_time": "2024-06-13T20:33:47.633219800Z"
    }
   },
   "id": "eb8a647db653ccd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **PART 2 : Exploring and Cleaning The Dataset**\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8f1aec82bb3212b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Tweet ID\n",
      "count  74681.000000\n",
      "mean    6432.640149\n",
      "std     3740.423819\n",
      "min        1.000000\n",
      "25%     3195.000000\n",
      "50%     6422.000000\n",
      "75%     9601.000000\n",
      "max    13200.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74681 entries, 0 to 74680\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet ID       74681 non-null  int64 \n",
      " 1   entity         74681 non-null  object\n",
      " 2   sentiment      74681 non-null  object\n",
      " 3   Tweet content  73995 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display basic statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Display data types and missing values\n",
    "print(df.info())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T20:33:53.357027400Z",
     "start_time": "2024-06-13T20:33:53.111957100Z"
    }
   },
   "id": "4b49b6bf8ee5a552"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T20:33:55.009188700Z",
     "start_time": "2024-06-13T20:33:54.891617500Z"
    }
   },
   "id": "12063c5418de2b57"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Tweet content'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T20:33:56.160954300Z",
     "start_time": "2024-06-13T20:33:56.045879900Z"
    }
   },
   "id": "838a3c336c983100"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **PART 3 : Establishing a preprocessing NLP pipeline**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "beb65ddc3578dd6d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Dell\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Dell\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Dell\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T20:34:00.937924200Z",
     "start_time": "2024-06-13T20:33:58.686374700Z"
    }
   },
   "id": "6978116dcca3df72"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "       Tweet ID       entity sentiment   \n0          2401  Borderlands  Positive  \\\n1          2401  Borderlands  Positive   \n2          2401  Borderlands  Positive   \n3          2401  Borderlands  Positive   \n4          2401  Borderlands  Positive   \n...         ...          ...       ...   \n74676      9200       Nvidia  Positive   \n74677      9200       Nvidia  Positive   \n74678      9200       Nvidia  Positive   \n74679      9200       Nvidia  Positive   \n74680      9200       Nvidia  Positive   \n\n                                           Tweet content   \n0      I am coming to the borders and I will kill you...  \\\n1      im getting on borderlands and i will kill you ...   \n2      im coming on borderlands and i will murder you...   \n3      im getting on borderlands 2 and i will murder ...   \n4      im getting into borderlands and i can murder y...   \n...                                                  ...   \n74676  Just realized that the Windows partition of my...   \n74677  Just realized that my Mac window partition is ...   \n74678  Just realized the windows partition of my Mac ...   \n74679  Just realized between the windows partition of...   \n74680  Just like the windows partition of my Mac is l...   \n\n                                                  tokens  \n0      [I, am, coming, to, the, borders, and, I, will...  \n1      [im, getting, on, borderlands, and, i, will, k...  \n2      [im, coming, on, borderlands, and, i, will, mu...  \n3      [im, getting, on, borderlands, 2, and, i, will...  \n4      [im, getting, into, borderlands, and, i, can, ...  \n...                                                  ...  \n74676  [Just, realized, that, the, Windows, partition...  \n74677  [Just, realized, that, my, Mac, window, partit...  \n74678  [Just, realized, the, windows, partition, of, ...  \n74679  [Just, realized, between, the, windows, partit...  \n74680  [Just, like, the, windows, partition, of, my, ...  \n\n[71655 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet ID</th>\n      <th>entity</th>\n      <th>sentiment</th>\n      <th>Tweet content</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>I am coming to the borders and I will kill you...</td>\n      <td>[I, am, coming, to, the, borders, and, I, will...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands and i will kill you ...</td>\n      <td>[im, getting, on, borderlands, and, i, will, k...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im coming on borderlands and i will murder you...</td>\n      <td>[im, coming, on, borderlands, and, i, will, mu...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands 2 and i will murder ...</td>\n      <td>[im, getting, on, borderlands, 2, and, i, will...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting into borderlands and i can murder y...</td>\n      <td>[im, getting, into, borderlands, and, i, can, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>74676</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized that the Windows partition of my...</td>\n      <td>[Just, realized, that, the, Windows, partition...</td>\n    </tr>\n    <tr>\n      <th>74677</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized that my Mac window partition is ...</td>\n      <td>[Just, realized, that, my, Mac, window, partit...</td>\n    </tr>\n    <tr>\n      <th>74678</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized the windows partition of my Mac ...</td>\n      <td>[Just, realized, the, windows, partition, of, ...</td>\n    </tr>\n    <tr>\n      <th>74679</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized between the windows partition of...</td>\n      <td>[Just, realized, between, the, windows, partit...</td>\n    </tr>\n    <tr>\n      <th>74680</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just like the windows partition of my Mac is l...</td>\n      <td>[Just, like, the, windows, partition, of, my, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>71655 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Tokenization\n",
    "df['tokens'] = df['Tweet content'].apply(word_tokenize)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T20:34:41.598402300Z",
     "start_time": "2024-06-13T20:34:00.902921200Z"
    }
   },
   "id": "c2e2521e141ece94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3a380198c91725b5"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "       Tweet ID       entity sentiment   \n0          2401  Borderlands  Positive  \\\n1          2401  Borderlands  Positive   \n2          2401  Borderlands  Positive   \n3          2401  Borderlands  Positive   \n4          2401  Borderlands  Positive   \n...         ...          ...       ...   \n74676      9200       Nvidia  Positive   \n74677      9200       Nvidia  Positive   \n74678      9200       Nvidia  Positive   \n74679      9200       Nvidia  Positive   \n74680      9200       Nvidia  Positive   \n\n                                           Tweet content   \n0      I am coming to the borders and I will kill you...  \\\n1      im getting on borderlands and i will kill you ...   \n2      im coming on borderlands and i will murder you...   \n3      im getting on borderlands 2 and i will murder ...   \n4      im getting into borderlands and i can murder y...   \n...                                                  ...   \n74676  Just realized that the Windows partition of my...   \n74677  Just realized that my Mac window partition is ...   \n74678  Just realized the windows partition of my Mac ...   \n74679  Just realized between the windows partition of...   \n74680  Just like the windows partition of my Mac is l...   \n\n                                                  tokens   \n0                             [coming, borders, kill, ,]  \\\n1                    [im, getting, borderlands, kill, ,]   \n2                   [im, coming, borderlands, murder, ,]   \n3               [im, getting, borderlands, 2, murder, ,]   \n4                  [im, getting, borderlands, murder, ,]   \n...                                                  ...   \n74676  [realized, Windows, partition, Mac, like, 6, y...   \n74677  [realized, Mac, window, partition, 6, years, b...   \n74678  [realized, windows, partition, Mac, 6, years, ...   \n74679  [realized, windows, partition, Mac, like, 6, y...   \n74680  [like, windows, partition, Mac, like, 6, years...   \n\n                                                 stemmed  \n0                                [come, border, kill, ,]  \n1                         [im, get, borderland, kill, ,]  \n2                      [im, come, borderland, murder, ,]  \n3                    [im, get, borderland, 2, murder, ,]  \n4                       [im, get, borderland, murder, ,]  \n...                                                  ...  \n74676  [realiz, window, partit, mac, like, 6, year, b...  \n74677  [realiz, mac, window, partit, 6, year, behind,...  \n74678  [realiz, window, partit, mac, 6, year, behind,...  \n74679  [realiz, window, partit, mac, like, 6, year, b...  \n74680  [like, window, partit, mac, like, 6, year, beh...  \n\n[71655 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet ID</th>\n      <th>entity</th>\n      <th>sentiment</th>\n      <th>Tweet content</th>\n      <th>tokens</th>\n      <th>stemmed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>I am coming to the borders and I will kill you...</td>\n      <td>[coming, borders, kill, ,]</td>\n      <td>[come, border, kill, ,]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands and i will kill you ...</td>\n      <td>[im, getting, borderlands, kill, ,]</td>\n      <td>[im, get, borderland, kill, ,]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im coming on borderlands and i will murder you...</td>\n      <td>[im, coming, borderlands, murder, ,]</td>\n      <td>[im, come, borderland, murder, ,]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands 2 and i will murder ...</td>\n      <td>[im, getting, borderlands, 2, murder, ,]</td>\n      <td>[im, get, borderland, 2, murder, ,]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting into borderlands and i can murder y...</td>\n      <td>[im, getting, borderlands, murder, ,]</td>\n      <td>[im, get, borderland, murder, ,]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>74676</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized that the Windows partition of my...</td>\n      <td>[realized, Windows, partition, Mac, like, 6, y...</td>\n      <td>[realiz, window, partit, mac, like, 6, year, b...</td>\n    </tr>\n    <tr>\n      <th>74677</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized that my Mac window partition is ...</td>\n      <td>[realized, Mac, window, partition, 6, years, b...</td>\n      <td>[realiz, mac, window, partit, 6, year, behind,...</td>\n    </tr>\n    <tr>\n      <th>74678</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized the windows partition of my Mac ...</td>\n      <td>[realized, windows, partition, Mac, 6, years, ...</td>\n      <td>[realiz, window, partit, mac, 6, year, behind,...</td>\n    </tr>\n    <tr>\n      <th>74679</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized between the windows partition of...</td>\n      <td>[realized, windows, partition, Mac, like, 6, y...</td>\n      <td>[realiz, window, partit, mac, like, 6, year, b...</td>\n    </tr>\n    <tr>\n      <th>74680</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just like the windows partition of my Mac is l...</td>\n      <td>[like, windows, partition, Mac, like, 6, years...</td>\n      <td>[like, window, partit, mac, like, 6, year, beh...</td>\n    </tr>\n  </tbody>\n</table>\n<p>71655 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmed'] = df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:25:07.511921300Z",
     "start_time": "2024-06-12T15:24:24.880774500Z"
    }
   },
   "id": "b875050bd2bbffa0"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "       Tweet ID       entity sentiment   \n0          2401  Borderlands  Positive  \\\n1          2401  Borderlands  Positive   \n2          2401  Borderlands  Positive   \n3          2401  Borderlands  Positive   \n4          2401  Borderlands  Positive   \n...         ...          ...       ...   \n74676      9200       Nvidia  Positive   \n74677      9200       Nvidia  Positive   \n74678      9200       Nvidia  Positive   \n74679      9200       Nvidia  Positive   \n74680      9200       Nvidia  Positive   \n\n                                           Tweet content   \n0      I am coming to the borders and I will kill you...  \\\n1      im getting on borderlands and i will kill you ...   \n2      im coming on borderlands and i will murder you...   \n3      im getting on borderlands 2 and i will murder ...   \n4      im getting into borderlands and i can murder y...   \n...                                                  ...   \n74676  Just realized that the Windows partition of my...   \n74677  Just realized that my Mac window partition is ...   \n74678  Just realized the windows partition of my Mac ...   \n74679  Just realized between the windows partition of...   \n74680  Just like the windows partition of my Mac is l...   \n\n                                                  tokens   \n0                             [coming, borders, kill, ,]  \\\n1                    [im, getting, borderlands, kill, ,]   \n2                   [im, coming, borderlands, murder, ,]   \n3               [im, getting, borderlands, 2, murder, ,]   \n4                  [im, getting, borderlands, murder, ,]   \n...                                                  ...   \n74676  [realized, Windows, partition, Mac, like, 6, y...   \n74677  [realized, Mac, window, partition, 6, years, b...   \n74678  [realized, windows, partition, Mac, 6, years, ...   \n74679  [realized, windows, partition, Mac, like, 6, y...   \n74680  [like, windows, partition, Mac, like, 6, years...   \n\n                                                 stemmed   \n0                                [come, border, kill, ,]  \\\n1                         [im, get, borderland, kill, ,]   \n2                      [im, come, borderland, murder, ,]   \n3                    [im, get, borderland, 2, murder, ,]   \n4                       [im, get, borderland, murder, ,]   \n...                                                  ...   \n74676  [realiz, window, partit, mac, like, 6, year, b...   \n74677  [realiz, mac, window, partit, 6, year, behind,...   \n74678  [realiz, window, partit, mac, 6, year, behind,...   \n74679  [realiz, window, partit, mac, like, 6, year, b...   \n74680  [like, window, partit, mac, like, 6, year, beh...   \n\n                                              lemmatized  \n0                              [coming, border, kill, ,]  \n1                     [im, getting, borderland, kill, ,]  \n2                    [im, coming, borderland, murder, ,]  \n3                [im, getting, borderland, 2, murder, ,]  \n4                   [im, getting, borderland, murder, ,]  \n...                                                  ...  \n74676  [realized, Windows, partition, Mac, like, 6, y...  \n74677  [realized, Mac, window, partition, 6, year, be...  \n74678  [realized, window, partition, Mac, 6, year, be...  \n74679  [realized, window, partition, Mac, like, 6, ye...  \n74680  [like, window, partition, Mac, like, 6, year, ...  \n\n[71655 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet ID</th>\n      <th>entity</th>\n      <th>sentiment</th>\n      <th>Tweet content</th>\n      <th>tokens</th>\n      <th>stemmed</th>\n      <th>lemmatized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>I am coming to the borders and I will kill you...</td>\n      <td>[coming, borders, kill, ,]</td>\n      <td>[come, border, kill, ,]</td>\n      <td>[coming, border, kill, ,]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands and i will kill you ...</td>\n      <td>[im, getting, borderlands, kill, ,]</td>\n      <td>[im, get, borderland, kill, ,]</td>\n      <td>[im, getting, borderland, kill, ,]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im coming on borderlands and i will murder you...</td>\n      <td>[im, coming, borderlands, murder, ,]</td>\n      <td>[im, come, borderland, murder, ,]</td>\n      <td>[im, coming, borderland, murder, ,]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands 2 and i will murder ...</td>\n      <td>[im, getting, borderlands, 2, murder, ,]</td>\n      <td>[im, get, borderland, 2, murder, ,]</td>\n      <td>[im, getting, borderland, 2, murder, ,]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting into borderlands and i can murder y...</td>\n      <td>[im, getting, borderlands, murder, ,]</td>\n      <td>[im, get, borderland, murder, ,]</td>\n      <td>[im, getting, borderland, murder, ,]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>74676</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized that the Windows partition of my...</td>\n      <td>[realized, Windows, partition, Mac, like, 6, y...</td>\n      <td>[realiz, window, partit, mac, like, 6, year, b...</td>\n      <td>[realized, Windows, partition, Mac, like, 6, y...</td>\n    </tr>\n    <tr>\n      <th>74677</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized that my Mac window partition is ...</td>\n      <td>[realized, Mac, window, partition, 6, years, b...</td>\n      <td>[realiz, mac, window, partit, 6, year, behind,...</td>\n      <td>[realized, Mac, window, partition, 6, year, be...</td>\n    </tr>\n    <tr>\n      <th>74678</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized the windows partition of my Mac ...</td>\n      <td>[realized, windows, partition, Mac, 6, years, ...</td>\n      <td>[realiz, window, partit, mac, 6, year, behind,...</td>\n      <td>[realized, window, partition, Mac, 6, year, be...</td>\n    </tr>\n    <tr>\n      <th>74679</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized between the windows partition of...</td>\n      <td>[realized, windows, partition, Mac, like, 6, y...</td>\n      <td>[realiz, window, partit, mac, like, 6, year, b...</td>\n      <td>[realized, window, partition, Mac, like, 6, ye...</td>\n    </tr>\n    <tr>\n      <th>74680</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just like the windows partition of my Mac is l...</td>\n      <td>[like, windows, partition, Mac, like, 6, years...</td>\n      <td>[like, window, partit, mac, like, 6, year, beh...</td>\n      <td>[like, window, partition, Mac, like, 6, year, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>71655 rows Ã— 7 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:25:17.443413Z",
     "start_time": "2024-06-12T15:25:07.511921300Z"
    }
   },
   "id": "c63df28f97a6dc66"
  },
  {
   "cell_type": "markdown",
   "source": [
    "we can notice the difference between stemmed column words and lemmatized, where as lemmatized is more correct and precise, so later on we will go on with it rather than stemmed "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b5d340922bcbca6"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "   Tweet ID       entity sentiment   \n0      2401  Borderlands  Positive  \\\n1      2401  Borderlands  Positive   \n2      2401  Borderlands  Positive   \n3      2401  Borderlands  Positive   \n4      2401  Borderlands  Positive   \n\n                                       Tweet content   \n0  I am coming to the borders and I will kill you...  \\\n1  im getting on borderlands and i will kill you ...   \n2  im coming on borderlands and i will murder you...   \n3  im getting on borderlands 2 and i will murder ...   \n4  im getting into borderlands and i can murder y...   \n\n                                     tokens   \n0                [coming, borders, kill, ,]  \\\n1       [im, getting, borderlands, kill, ,]   \n2      [im, coming, borderlands, murder, ,]   \n3  [im, getting, borderlands, 2, murder, ,]   \n4     [im, getting, borderlands, murder, ,]   \n\n                               stemmed   \n0              [come, border, kill, ,]  \\\n1       [im, get, borderland, kill, ,]   \n2    [im, come, borderland, murder, ,]   \n3  [im, get, borderland, 2, murder, ,]   \n4     [im, get, borderland, murder, ,]   \n\n                                lemmatized  encoded_sentiment  \n0                [coming, border, kill, ,]                  3  \n1       [im, getting, borderland, kill, ,]                  3  \n2      [im, coming, borderland, murder, ,]                  3  \n3  [im, getting, borderland, 2, murder, ,]                  3  \n4     [im, getting, borderland, murder, ,]                  3  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet ID</th>\n      <th>entity</th>\n      <th>sentiment</th>\n      <th>Tweet content</th>\n      <th>tokens</th>\n      <th>stemmed</th>\n      <th>lemmatized</th>\n      <th>encoded_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>I am coming to the borders and I will kill you...</td>\n      <td>[coming, borders, kill, ,]</td>\n      <td>[come, border, kill, ,]</td>\n      <td>[coming, border, kill, ,]</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands and i will kill you ...</td>\n      <td>[im, getting, borderlands, kill, ,]</td>\n      <td>[im, get, borderland, kill, ,]</td>\n      <td>[im, getting, borderland, kill, ,]</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im coming on borderlands and i will murder you...</td>\n      <td>[im, coming, borderlands, murder, ,]</td>\n      <td>[im, come, borderland, murder, ,]</td>\n      <td>[im, coming, borderland, murder, ,]</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands 2 and i will murder ...</td>\n      <td>[im, getting, borderlands, 2, murder, ,]</td>\n      <td>[im, get, borderland, 2, murder, ,]</td>\n      <td>[im, getting, borderland, 2, murder, ,]</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting into borderlands and i can murder y...</td>\n      <td>[im, getting, borderlands, murder, ,]</td>\n      <td>[im, get, borderland, murder, ,]</td>\n      <td>[im, getting, borderland, murder, ,]</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the sentiment labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_sentiment'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:25:23.161994700Z",
     "start_time": "2024-06-12T15:25:23.096993700Z"
    }
   },
   "id": "b0d14e2cb6ac4874"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **PART 4 : Encode My Data vectors**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "735888e2c372ea8"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Importing necessary libraries for encoding\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:25:27.875013500Z",
     "start_time": "2024-06-12T15:25:27.343014200Z"
    }
   },
   "id": "8866dadc68701117"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Prepare the text data for vectorization\n",
    "df['processed'] = df['lemmatized'].apply(lambda x: ' '.join(x))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:25:31.714509400Z",
     "start_time": "2024-06-12T15:25:31.584544800Z"
    }
   },
   "id": "b9a9bfb1a97a36d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Applying Bag of Words encoding using CountVectorizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65540375bfef90c4"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6229)\t1\n",
      "  (0, 4528)\t1\n",
      "  (0, 15061)\t1\n",
      "  (1, 15061)\t1\n",
      "  (1, 13517)\t1\n",
      "  (1, 11463)\t1\n",
      "  (1, 4532)\t1\n",
      "  (2, 6229)\t1\n",
      "  (2, 13517)\t1\n",
      "  (2, 4532)\t1\n",
      "  (2, 17695)\t1\n",
      "  (3, 13517)\t1\n",
      "  (3, 11463)\t1\n",
      "  (3, 4532)\t1\n",
      "  (3, 17695)\t1\n",
      "  (4, 13517)\t1\n",
      "  (4, 11463)\t1\n",
      "  (4, 4532)\t1\n",
      "  (4, 17695)\t1\n",
      "  (5, 24487)\t1\n",
      "  (5, 13147)\t1\n",
      "  (5, 16449)\t1\n",
      "  (5, 24282)\t1\n",
      "  (5, 11032)\t1\n",
      "  (5, 15170)\t1\n",
      "  :\t:\n",
      "  (71652, 18504)\t1\n",
      "  (71652, 19426)\t1\n",
      "  (71653, 15779)\t1\n",
      "  (71653, 29296)\t1\n",
      "  (71653, 10995)\t1\n",
      "  (71653, 9585)\t1\n",
      "  (71653, 13406)\t1\n",
      "  (71653, 3868)\t1\n",
      "  (71653, 16328)\t1\n",
      "  (71653, 21413)\t1\n",
      "  (71653, 5230)\t1\n",
      "  (71653, 18386)\t1\n",
      "  (71653, 8627)\t1\n",
      "  (71653, 28691)\t1\n",
      "  (71653, 18504)\t1\n",
      "  (71653, 19426)\t1\n",
      "  (71654, 15779)\t2\n",
      "  (71654, 29296)\t1\n",
      "  (71654, 13406)\t1\n",
      "  (71654, 3868)\t1\n",
      "  (71654, 16328)\t1\n",
      "  (71654, 18386)\t1\n",
      "  (71654, 8627)\t1\n",
      "  (71654, 28691)\t1\n",
      "  (71654, 19426)\t1\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_bow = vectorizer_bow.fit_transform(df['processed'])\n",
    "print(X_bow)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:25:36.346646300Z",
     "start_time": "2024-06-12T15:25:34.142504800Z"
    }
   },
   "id": "1c65522171e9150c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Applying TF-IDF encoding using TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f09a9c241ed3957"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15061)\t0.4999099084258951\n",
      "  (0, 4528)\t0.6968320412638016\n",
      "  (0, 6229)\t0.514310402116987\n",
      "  (1, 4532)\t0.5482129129572333\n",
      "  (1, 11463)\t0.4401933535571443\n",
      "  (1, 13517)\t0.4969403985067297\n",
      "  (1, 15061)\t0.5086675278441045\n",
      "  (2, 17695)\t0.6022099540306071\n",
      "  (2, 4532)\t0.48291584796682496\n",
      "  (2, 13517)\t0.4377503489279728\n",
      "  (2, 6229)\t0.4609881636901045\n",
      "  (3, 17695)\t0.6218438386378004\n",
      "  (3, 4532)\t0.4986603802026432\n",
      "  (3, 11463)\t0.4004046235674972\n",
      "  (3, 13517)\t0.45202234788794615\n",
      "  (4, 17695)\t0.6218438386378004\n",
      "  (4, 4532)\t0.4986603802026432\n",
      "  (4, 11463)\t0.4004046235674972\n",
      "  (4, 13517)\t0.45202234788794615\n",
      "  (5, 17313)\t0.3124584818547383\n",
      "  (5, 6196)\t0.09002036222277351\n",
      "  (5, 26982)\t0.11716369000954954\n",
      "  (5, 19838)\t0.11632096453987982\n",
      "  (5, 9304)\t0.1795742732553685\n",
      "  (5, 16362)\t0.15560290451122197\n",
      "  :\t:\n",
      "  (71652, 13406)\t0.2672452902238448\n",
      "  (71652, 29296)\t0.195866072535715\n",
      "  (71653, 19426)\t0.4056204252707394\n",
      "  (71653, 18504)\t0.19357653193469976\n",
      "  (71653, 28691)\t0.30712948620018754\n",
      "  (71653, 8627)\t0.2764835872170336\n",
      "  (71653, 18386)\t0.3143559231945002\n",
      "  (71653, 5230)\t0.2623465369683012\n",
      "  (71653, 21413)\t0.31379370568791903\n",
      "  (71653, 16328)\t0.3191661420305989\n",
      "  (71653, 3868)\t0.26932980958247377\n",
      "  (71653, 13406)\t0.24439024983204788\n",
      "  (71653, 9585)\t0.19513967685733932\n",
      "  (71653, 10995)\t0.18556907125274838\n",
      "  (71653, 29296)\t0.17911544244813946\n",
      "  (71653, 15779)\t0.1496221166233779\n",
      "  (71654, 19426)\t0.45639303959991645\n",
      "  (71654, 28691)\t0.34557372120524693\n",
      "  (71654, 8627)\t0.31109179150740685\n",
      "  (71654, 18386)\t0.35370471101699086\n",
      "  (71654, 16328)\t0.359117038057185\n",
      "  (71654, 3868)\t0.3030425560255412\n",
      "  (71654, 13406)\t0.2749812435973436\n",
      "  (71654, 29296)\t0.20153581063780648\n",
      "  (71654, 15779)\t0.33670144964488635\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(df['processed'])\n",
    "print(X_tfidf)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:25:41.513339800Z",
     "start_time": "2024-06-12T15:25:37.819649300Z"
    }
   },
   "id": "5ed05abbd14f8c8b"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Word2Vec (CBOW and Skip Gram)\n",
    "# CBOW\n",
    "word2vec_cbow = Word2Vec(sentences=df['lemmatized'], vector_size=100, window=5, min_count=1, sg=0)\n",
    "# Skip Gram\n",
    "word2vec_sg = Word2Vec(sentences=df['lemmatized'], vector_size=100, window=5, min_count=1, sg=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:26:19.441987Z",
     "start_time": "2024-06-12T15:25:42.961808400Z"
    }
   },
   "id": "6eb373d77083a4d8"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Function to average word vectors for a document\n",
    "def get_avg_word2vec(tokens_list, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tokens_list if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if len(vectors) > 0 else np.zeros(vector_size)\n",
    "\n",
    "# Apply Word2Vec to the data\n",
    "df['word2vec_cbow'] = df['lemmatized'].apply(lambda x: get_avg_word2vec(x, word2vec_cbow, 100))\n",
    "df['word2vec_sg'] = df['lemmatized'].apply(lambda x: get_avg_word2vec(x, word2vec_sg, 100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:26:51.196300500Z",
     "start_time": "2024-06-12T15:26:34.188810800Z"
    }
   },
   "id": "575ad1e92a30e49e"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Prepare features and target variable\n",
    "X_cbow = np.array(df['word2vec_cbow'].tolist())\n",
    "X_sg = np.array(df['word2vec_sg'].tolist())\n",
    "y = df['encoded_sentiment']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:26:51.498146100Z",
     "start_time": "2024-06-12T15:26:51.199303Z"
    }
   },
   "id": "c6f5be8a2ae482fc"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec CBOW Vectors:\n",
      " [[ 0.46213767  0.18220426  0.46943012 ...  0.90106708  0.69632769\n",
      "  -0.38926613]\n",
      " [ 1.27437901 -0.22491868  0.73573428 ...  1.50841641  0.73503077\n",
      "  -0.19457839]\n",
      " [ 0.83647525  0.1246312   0.58204043 ...  1.22404325  0.58970731\n",
      "  -0.22111849]\n",
      " ...\n",
      " [ 0.46233511 -0.3661783   0.60747749 ...  0.73391789  0.18370862\n",
      "  -0.24627589]\n",
      " [ 0.65227646 -0.13276321  0.52342546 ...  1.03743231  0.09122739\n",
      "  -0.11251325]\n",
      " [ 0.77817702 -0.04158317  0.49581754 ...  1.15533543 -0.1068337\n",
      "  -0.2166041 ]]\n",
      "\n",
      "Word2Vec Skip Gram Vectors:\n",
      " [[ 0.43595627  0.29788211  0.28384411 ...  0.13823794 -0.00959883\n",
      "  -0.21663749]\n",
      " [ 0.48288912  0.15907834  0.53743398 ...  0.3014074  -0.1525337\n",
      "  -0.08938862]\n",
      " [ 0.54036981  0.23761804  0.29388112 ...  0.30156958 -0.03040371\n",
      "  -0.11556394]\n",
      " ...\n",
      " [ 0.18826284  0.21562345  0.47357163 ...  0.11015111 -0.00261236\n",
      "  -0.371979  ]\n",
      " [ 0.27062643  0.29427326  0.4187488  ...  0.23890769 -0.03256583\n",
      "  -0.33701232]\n",
      " [ 0.37442219  0.3078306   0.41352525 ...  0.30606386 -0.0912725\n",
      "  -0.27777708]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(\"Bag of Words Vectors:\\n\", X_bow.toarray())\n",
    "# print(\"\\nTF-IDF Vectors:\\n\", X_tfidf.toarray())\n",
    "print(\"\\nWord2Vec CBOW Vectors:\\n\", X_cbow)\n",
    "print(\"\\nWord2Vec Skip Gram Vectors:\\n\", X_sg)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:29:17.242939900Z",
     "start_time": "2024-06-12T15:29:17.205048100Z"
    }
   },
   "id": "7d4d9b6f3c4b94c4"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:29:35.578104500Z",
     "start_time": "2024-06-12T15:29:35.513564400Z"
    }
   },
   "id": "183bf9438e99b31c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **PART 5 : Training Models**\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc2666588a6947ec"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Importing necessary libraries for machine learning\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:30:36.020594600Z",
     "start_time": "2024-06-12T15:30:33.824326800Z"
    }
   },
   "id": "9e0dedfe0099bc49"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train_cbow, X_test_cbow, y_train, y_test = train_test_split(X_cbow, y, test_size=0.2, random_state=42)\n",
    "X_train_sg, X_test_sg, y_train, y_test = train_test_split(X_sg, y, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:30:36.211588Z",
     "start_time": "2024-06-12T15:30:36.008596Z"
    }
   },
   "id": "1ceca34a18db82be"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "svm = SVC()\n",
    "lr = LogisticRegression()\n",
    "ada = AdaBoostClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:30:36.247590900Z",
     "start_time": "2024-06-12T15:30:36.179719400Z"
    }
   },
   "id": "dc202f9218c691ed"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Function to train and evaluate a model\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    return accuracy, f1, precision, recall"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:30:55.063008800Z",
     "start_time": "2024-06-12T15:30:54.988812400Z"
    }
   },
   "id": "d010a8faae7e0334"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate the four languages models by using standards metrics (Accuracy, Loss, F1 Score, etc) and other metrics like blue score "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3a5b6d9b2eae851"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Embeddings:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m results_cbow \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model_name, model \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSVM\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLogistic Regression\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAdaBoost\u001B[39m\u001B[38;5;124m'\u001B[39m], [svm, lr, ada]):\n\u001B[1;32m----> 5\u001B[0m     accuracy, f1, precision, recall \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train_cbow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test_cbow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     results_cbow[model_name] \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m: accuracy, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mF1 Score\u001B[39m\u001B[38;5;124m'\u001B[39m: f1, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPrecision\u001B[39m\u001B[38;5;124m'\u001B[39m: precision, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRecall\u001B[39m\u001B[38;5;124m'\u001B[39m: recall}\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: Accuracy=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, F1 Score=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mf1\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Precision=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprecision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Recall=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrecall\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[30], line 3\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[1;34m(model, X_train, X_test, y_train, y_test)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_and_evaluate\u001B[39m(model, X_train, X_test, y_train, y_test):\n\u001B[1;32m----> 3\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m      5\u001B[0m     accuracy \u001B[38;5;241m=\u001B[39m accuracy_score(y_test, y_pred)\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1150\u001B[0m     )\n\u001B[0;32m   1151\u001B[0m ):\n\u001B[1;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:250\u001B[0m, in \u001B[0;36mBaseLibSVM.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    247\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LibSVM]\u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    249\u001B[0m seed \u001B[38;5;241m=\u001B[39m rnd\u001B[38;5;241m.\u001B[39mrandint(np\u001B[38;5;241m.\u001B[39miinfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmax)\n\u001B[1;32m--> 250\u001B[0m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msolver_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    251\u001B[0m \u001B[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001B[39;00m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape_fit_ \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(X, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m (n_samples,)\n",
      "File \u001B[1;32mC:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:329\u001B[0m, in \u001B[0;36mBaseLibSVM._dense_fit\u001B[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001B[0m\n\u001B[0;32m    315\u001B[0m libsvm\u001B[38;5;241m.\u001B[39mset_verbosity_wrap(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose)\n\u001B[0;32m    317\u001B[0m \u001B[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001B[39;00m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;66;03m# add other parameters to __init__\u001B[39;00m\n\u001B[0;32m    319\u001B[0m (\n\u001B[0;32m    320\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupport_,\n\u001B[0;32m    321\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupport_vectors_,\n\u001B[0;32m    322\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_support,\n\u001B[0;32m    323\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdual_coef_,\n\u001B[0;32m    324\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintercept_,\n\u001B[0;32m    325\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_probA,\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_probB,\n\u001B[0;32m    327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_status_,\n\u001B[0;32m    328\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_iter,\n\u001B[1;32m--> 329\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[43mlibsvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    330\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    331\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    332\u001B[0m \u001B[43m    \u001B[49m\u001B[43msvm_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msolver_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    333\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    334\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001B[39;49;00m\n\u001B[0;32m    335\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_class_weight\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    336\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkernel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkernel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    337\u001B[0m \u001B[43m    \u001B[49m\u001B[43mC\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    338\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnu\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnu\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    339\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprobability\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprobability\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    340\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdegree\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdegree\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    341\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshrinking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshrinking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    343\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcache_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcoef0\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcoef0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gamma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    346\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_seed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_warn_from_fit_status()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Train and evaluate models using CBOW embeddings\n",
    "print(\"CBOW Embeddings:\")\n",
    "results_cbow = {}\n",
    "for model_name, model in zip(['SVM', 'Logistic Regression', 'AdaBoost'], [svm, lr, ada]):\n",
    "    accuracy, f1, precision, recall = train_and_evaluate(model, X_train_cbow, X_test_cbow, y_train, y_test)\n",
    "    results_cbow[model_name] = {'Accuracy': accuracy, 'F1 Score': f1, 'Precision': precision, 'Recall': recall}\n",
    "    print(f\"{model_name}: Accuracy={accuracy}, F1 Score={f1}, Precision={precision}, Recall={recall}\")\n",
    "\n",
    "# Train and evaluate models using Skip Gram embeddings\n",
    "print(\"\\nSkip Gram Embeddings:\")\n",
    "results_sg = {}\n",
    "for model_name, model in zip(['SVM', 'Logistic Regression', 'AdaBoost'], [svm, lr, ada]):\n",
    "    accuracy, f1, precision, recall = train_and_evaluate(model, X_train_sg, X_test_sg, y_train, y_test)\n",
    "    results_sg[model_name] = {'Accuracy': accuracy, 'F1 Score': f1, 'Precision': precision, 'Recall': recall}\n",
    "    print(f\"{model_name}: Accuracy={accuracy}, F1 Score={f1}, Precision={precision}, Recall={recall}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T15:50:31.689365400Z",
     "start_time": "2024-06-12T15:31:16.581320600Z"
    }
   },
   "id": "82c0ec51a91a400"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Best Model Selection:\n",
    "\n",
    "**Best Model by Embedding Type:**\n",
    "\n",
    "- **CBOW**: \n",
    "  - **Model**: SVM\n",
    "  - **Metrics**:\n",
    "    - Accuracy: 0.5383\n",
    "    - F1 Score: 0.5126\n",
    "    - Precision: 0.5323\n",
    "    - Recall: 0.5334\n",
    "\n",
    "- **Skip Gram**:\n",
    "  - **Model**: SVM\n",
    "  - **Metrics**:\n",
    "    - Accuracy: 0.5887\n",
    "    - F1 Score: 0.5763\n",
    "    - Precision: 0.57945\n",
    "    - Recall: 0.5812\n",
    "\n",
    "**Overall Best Model:**\n",
    "\n",
    "Comparing the best models from both embeddings, SVM with Skip Gram embeddings has the highest performance across all metrics:\n",
    "- **Accuracy**: 0.5845\n",
    "- **F1 Score**: 0.5723\n",
    "- **Precision**: 0.5734\n",
    "- **Recall**: 0.5823\n",
    "\n",
    "### Interpret the Obtained Results:\n",
    "\n",
    "**SVM with Skip Gram Embeddings**: This combination provides the highest accuracy and the best balance between precision and recall, as indicated by the F1 Score. This suggests that the SVM model is effective in distinguishing between different sentiment classes when using Skip Gram embeddings, which capture the context around each word more effectively than CBOW for this dataset.\n",
    "\n",
    "**Conclusion**: The SVM model with Skip Gram embeddings is the best choice for this Twitter Sentiment Analysis task. It has the highest accuracy and F1 Score, indicating it makes the most reliable and balanced predictions. The rich contextual information provided by Skip Gram embeddings enhances SVM's classification capabilities, making it the optimal model for this dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62334be9a740ae78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a71685a551eccbf6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
